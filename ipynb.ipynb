{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cansakiroglu/miniconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset docred (/Users/cansakiroglu/.cache/huggingface/datasets/docred/default/0.0.0/5c49322404435e948decc4551bb3378a75e947e3f0199d9401eb79586468b179)\n",
      "100%|██████████| 4/4 [00:00<00:00, 120.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['title', 'sents', 'vertexSet', 'labels'],\n",
      "        num_rows: 998\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['title', 'sents', 'vertexSet', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    train_annotated: Dataset({\n",
      "        features: ['title', 'sents', 'vertexSet', 'labels'],\n",
      "        num_rows: 3053\n",
      "    })\n",
      "    train_distant: Dataset({\n",
      "        features: ['title', 'sents', 'vertexSet', 'labels'],\n",
      "        num_rows: 101873\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"docred\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb Cell 2\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m sents_data[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m triplets_data[key] \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m dataset[key]:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     triplets \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(row[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mrelation_text\u001b[39m\u001b[39m'\u001b[39m])):\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/arrow_dataset.py:2362\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2360\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(pa_subtable\u001b[39m.\u001b[39mnum_rows):\n\u001b[1;32m   2361\u001b[0m             pa_subtable_ex \u001b[39m=\u001b[39m pa_subtable\u001b[39m.\u001b[39mslice(i, \u001b[39m1\u001b[39m)\n\u001b[0;32m-> 2362\u001b[0m             formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2363\u001b[0m                 pa_subtable_ex,\n\u001b[1;32m   2364\u001b[0m                 \u001b[39m0\u001b[39m,\n\u001b[1;32m   2365\u001b[0m                 formatter\u001b[39m=\u001b[39mformatter,\n\u001b[1;32m   2366\u001b[0m                 format_columns\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m   2367\u001b[0m                 output_all_columns\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m   2368\u001b[0m             )\n\u001b[1;32m   2369\u001b[0m             \u001b[39myield\u001b[39;00m formatted_output\n\u001b[1;32m   2370\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/formatting/formatting.py:624\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    622\u001b[0m python_formatter \u001b[39m=\u001b[39m PythonFormatter(features\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m format_columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[39mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[39m=\u001b[39mquery_type)\n\u001b[1;32m    625\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    626\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/formatting/formatting.py:396\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable, query_type: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 396\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m    397\u001b[0m     \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    398\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/formatting/formatting.py:431\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlazy:\n\u001b[1;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m LazyRow(pa_table, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 431\u001b[0m row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_arrow_extractor()\u001b[39m.\u001b[39mextract_row(pa_table)\n\u001b[1;32m    432\u001b[0m row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_features_decoder\u001b[39m.\u001b[39mdecode_row(row)\n\u001b[1;32m    433\u001b[0m \u001b[39mreturn\u001b[39;00m row\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/formatting/formatting.py:144\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_row\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[39mreturn\u001b[39;00m _unnest(pa_table\u001b[39m.\u001b[39mto_pydict())\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/pyarrow/table.pxi:4022\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.to_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/pyarrow/table.pxi:1284\u001b[0m, in \u001b[0;36mpyarrow.lib.ChunkedArray.to_pylist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/pyarrow/array.pxi:1498\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.to_pylist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/pyarrow/scalar.pxi:675\u001b[0m, in \u001b[0;36mpyarrow.lib.ListScalar.as_py\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/pyarrow/array.pxi:1498\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.to_pylist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/pyarrow/scalar.pxi:675\u001b[0m, in \u001b[0;36mpyarrow.lib.ListScalar.as_py\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/pyarrow/array.pxi:1498\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.to_pylist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/pyarrow/scalar.pxi:748\u001b[0m, in \u001b[0;36mpyarrow.lib.StructScalar.as_py\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen _collections_abc>:786\u001b[0m, in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare the datasets\n",
    "sents_data = {}\n",
    "triplets_data = {}\n",
    "for key in dataset.keys():\n",
    "    sents_data[key] = []\n",
    "    triplets_data[key] = []\n",
    "    for row in dataset[key]:\n",
    "        triplets = []\n",
    "        for i in range(len(row['labels']['relation_text'])):\n",
    "            triplets.append([\"<triplet>\", row['vertexSet'][row['labels']['head'][i]][0]['name'], \"<subj>\", row['vertexSet'][row['labels']['tail'][i]][0]['name'], \"<obj>\", row['labels']['relation_text'][i]])\n",
    "        sents_data[key].append(row['sents'])\n",
    "        triplets_data[key].append(triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Skai',\n",
       "  'TV',\n",
       "  'is',\n",
       "  'a',\n",
       "  'Greek',\n",
       "  'free',\n",
       "  '-',\n",
       "  'to',\n",
       "  '-',\n",
       "  'air',\n",
       "  'television',\n",
       "  'network',\n",
       "  'based',\n",
       "  'in',\n",
       "  'Piraeus',\n",
       "  '.'],\n",
       " ['It',\n",
       "  'is',\n",
       "  'part',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Skai',\n",
       "  'Group',\n",
       "  ',',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'largest',\n",
       "  'media',\n",
       "  'groups',\n",
       "  'in',\n",
       "  'the',\n",
       "  'country',\n",
       "  '.'],\n",
       " ['It',\n",
       "  'was',\n",
       "  'relaunched',\n",
       "  'in',\n",
       "  'its',\n",
       "  'present',\n",
       "  'form',\n",
       "  'on',\n",
       "  '1st',\n",
       "  'of',\n",
       "  'April',\n",
       "  '2006',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Athens',\n",
       "  'metropolitan',\n",
       "  'area',\n",
       "  ',',\n",
       "  'and',\n",
       "  'gradually',\n",
       "  'spread',\n",
       "  'its',\n",
       "  'coverage',\n",
       "  'nationwide',\n",
       "  '.'],\n",
       " ['Besides',\n",
       "  'digital',\n",
       "  'terrestrial',\n",
       "  'transmission',\n",
       "  ',',\n",
       "  'it',\n",
       "  'is',\n",
       "  'available',\n",
       "  'on',\n",
       "  'the',\n",
       "  'subscription',\n",
       "  '-',\n",
       "  'based',\n",
       "  'encrypted',\n",
       "  'services',\n",
       "  'of',\n",
       "  'Nova',\n",
       "  'and',\n",
       "  'Cosmote',\n",
       "  'TV',\n",
       "  '.'],\n",
       " ['Skai',\n",
       "  'TV',\n",
       "  'is',\n",
       "  'also',\n",
       "  'a',\n",
       "  'member',\n",
       "  'of',\n",
       "  'Digea',\n",
       "  ',',\n",
       "  'a',\n",
       "  'consortium',\n",
       "  'of',\n",
       "  'private',\n",
       "  'television',\n",
       "  'networks',\n",
       "  'introducing',\n",
       "  'digital',\n",
       "  'terrestrial',\n",
       "  'transmission',\n",
       "  'in',\n",
       "  'Greece',\n",
       "  '.'],\n",
       " ['At',\n",
       "  'launch',\n",
       "  ',',\n",
       "  'Skai',\n",
       "  'TV',\n",
       "  'opted',\n",
       "  'for',\n",
       "  'dubbing',\n",
       "  'all',\n",
       "  'foreign',\n",
       "  'language',\n",
       "  'content',\n",
       "  'into',\n",
       "  'Greek',\n",
       "  ',',\n",
       "  'instead',\n",
       "  'of',\n",
       "  'using',\n",
       "  'subtitles',\n",
       "  '.'],\n",
       " ['This',\n",
       "  'is',\n",
       "  'very',\n",
       "  'uncommon',\n",
       "  'in',\n",
       "  'Greece',\n",
       "  'for',\n",
       "  'anything',\n",
       "  'except',\n",
       "  'documentaries',\n",
       "  '(',\n",
       "  'using',\n",
       "  'voiceover',\n",
       "  'dubbing',\n",
       "  ')',\n",
       "  'and',\n",
       "  'children',\n",
       "  \"'s\",\n",
       "  'programmes',\n",
       "  '(',\n",
       "  'using',\n",
       "  'lip',\n",
       "  '-',\n",
       "  'synced',\n",
       "  'dubbing',\n",
       "  ')',\n",
       "  ',',\n",
       "  'so',\n",
       "  'after',\n",
       "  'intense',\n",
       "  'criticism',\n",
       "  'the',\n",
       "  'station',\n",
       "  'switched',\n",
       "  'to',\n",
       "  'using',\n",
       "  'subtitles',\n",
       "  'for',\n",
       "  'almost',\n",
       "  'all',\n",
       "  'foreign',\n",
       "  'shows',\n",
       "  '.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_data['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<triplet>', 'Piraeus', '<subj>', 'Greece', '<obj>', 'country'],\n",
       " ['<triplet>', 'Skai Group', '<subj>', 'Greece', '<obj>', 'country'],\n",
       " ['<triplet>', 'Athens', '<subj>', 'Greece', '<obj>', 'country'],\n",
       " ['<triplet>',\n",
       "  'Skai TV',\n",
       "  '<subj>',\n",
       "  'Piraeus',\n",
       "  '<obj>',\n",
       "  'headquarters location'],\n",
       " ['<triplet>', 'Skai TV', '<subj>', 'Skai Group', '<obj>', 'owned by'],\n",
       " ['<triplet>',\n",
       "  'Skai TV',\n",
       "  '<subj>',\n",
       "  'Athens',\n",
       "  '<obj>',\n",
       "  'headquarters location'],\n",
       " ['<triplet>', 'Skai TV', '<subj>', 'Greece', '<obj>', 'country']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_data['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triplets_data['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<triplet>',\n",
       "  'Zest Airways, Inc.',\n",
       "  '<subj>',\n",
       "  'Pasay City',\n",
       "  '<obj>',\n",
       "  'headquarters location'],\n",
       " ['<triplet>',\n",
       "  'Zest Airways, Inc.',\n",
       "  '<subj>',\n",
       "  'Philippines',\n",
       "  '<obj>',\n",
       "  'country'],\n",
       " ['<triplet>', 'Zest Air', '<subj>', 'Philippines', '<obj>', 'country'],\n",
       " ['<triplet>', 'Pasay City', '<subj>', 'Philippines', '<obj>', 'country'],\n",
       " ['<triplet>',\n",
       "  'Pasay City',\n",
       "  '<subj>',\n",
       "  'Metro Manila',\n",
       "  '<obj>',\n",
       "  'located in the administrative territorial entity'],\n",
       " ['<triplet>',\n",
       "  'Philippines',\n",
       "  '<subj>',\n",
       "  'Metro Manila',\n",
       "  '<obj>',\n",
       "  'contains administrative territorial entity'],\n",
       " ['<triplet>', 'Manila', '<subj>', 'Philippines', '<obj>', 'country'],\n",
       " ['<triplet>',\n",
       "  'Metro Manila',\n",
       "  '<subj>',\n",
       "  'Pasay City',\n",
       "  '<obj>',\n",
       "  'contains administrative territorial entity'],\n",
       " ['<triplet>',\n",
       "  'Metro Manila',\n",
       "  '<subj>',\n",
       "  'Philippines',\n",
       "  '<obj>',\n",
       "  'located in the administrative territorial entity'],\n",
       " ['<triplet>', 'Metro Manila', '<subj>', 'Philippines', '<obj>', 'country'],\n",
       " ['<triplet>',\n",
       "  'Ninoy Aquino International Airport',\n",
       "  '<subj>',\n",
       "  'Pasay City',\n",
       "  '<obj>',\n",
       "  'located in the administrative territorial entity'],\n",
       " ['<triplet>',\n",
       "  'Ninoy Aquino International Airport',\n",
       "  '<subj>',\n",
       "  'Philippines',\n",
       "  '<obj>',\n",
       "  'country'],\n",
       " ['<triplet>', 'Asian Spirit', '<subj>', 'Philippines', '<obj>', 'country']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_data['train_annotated'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<triplet>',\n",
       "  'North Korea',\n",
       "  '<subj>',\n",
       "  'Kim Il-sung',\n",
       "  '<obj>',\n",
       "  'head of state'],\n",
       " ['<triplet>',\n",
       "  'Kim Il-sung',\n",
       "  '<subj>',\n",
       "  'North Korea',\n",
       "  '<obj>',\n",
       "  'country of citizenship'],\n",
       " ['<triplet>',\n",
       "  'Kim Il-sung',\n",
       "  '<subj>',\n",
       "  \"Workers' Party of Korea\",\n",
       "  '<obj>',\n",
       "  'member of political party'],\n",
       " ['<triplet>',\n",
       "  'State Affairs Commission',\n",
       "  '<subj>',\n",
       "  'North Korea',\n",
       "  '<obj>',\n",
       "  'country'],\n",
       " ['<triplet>',\n",
       "  'State Affairs Commission',\n",
       "  '<subj>',\n",
       "  '2016',\n",
       "  '<obj>',\n",
       "  'inception'],\n",
       " ['<triplet>',\n",
       "  'State Affairs Commission',\n",
       "  '<subj>',\n",
       "  'National Defense Commission',\n",
       "  '<obj>',\n",
       "  'replaces'],\n",
       " ['<triplet>',\n",
       "  'National Defense Commission',\n",
       "  '<subj>',\n",
       "  'North Korea',\n",
       "  '<obj>',\n",
       "  'country'],\n",
       " ['<triplet>',\n",
       "  'National Defense Commission',\n",
       "  '<subj>',\n",
       "  'State Affairs Commission',\n",
       "  '<obj>',\n",
       "  'replaced by'],\n",
       " ['<triplet>',\n",
       "  \"Workers' Party of Korea\",\n",
       "  '<subj>',\n",
       "  'North Korea',\n",
       "  '<obj>',\n",
       "  'country'],\n",
       " ['<triplet>',\n",
       "  \"Workers' Party of Korea\",\n",
       "  '<subj>',\n",
       "  'Kim Il-sung',\n",
       "  '<obj>',\n",
       "  'founded by'],\n",
       " ['<triplet>',\n",
       "  \"Workers' Party of Korea\",\n",
       "  '<subj>',\n",
       "  'Kim Il-sung',\n",
       "  '<obj>',\n",
       "  'chairperson']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_data['train_distant'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_data['train'] = sents_data['train_annotated'] + sents_data['train_distant']\n",
    "triplets_data['train'] = triplets_data['train_annotated'] + triplets_data['train_distant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_data['train'] = [' '.join([' '.join(triplet) for triplet in triplets]) for triplets in triplets_data['train']]\n",
    "triplets_data['validation'] = [' '.join([' '.join(triplet) for triplet in triplets]) for triplets in triplets_data['validation']]\n",
    "triplets_data['test'] = [' '.join([' '.join(triplet) for triplet in triplets]) for triplets in triplets_data['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_data['train'] = [' '.join([' '.join(sent) for sent in sents]) for sents in sents_data['train']]\n",
    "sents_data['validation'] = [' '.join([' '.join(sent) for sent in sents]) for sents in sents_data['validation']]\n",
    "sents_data['test'] = [' '.join([' '.join(sent) for sent in sents]) for sents in sents_data['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104926"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triplets_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "w2v = gensim.downloader.load('word2vec-google-news-300')\n",
    "w2v.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset docred (/Users/cansakiroglu/.cache/huggingface/datasets/docred/default/0.0.0/5c49322404435e948decc4551bb3378a75e947e3f0199d9401eb79586468b179)\n",
      "100%|██████████| 4/4 [00:00<00:00, 322.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================================================\n",
    "# ----- DATA FORMAT -----\n",
    "# sents_data_train --> texts for training (X)\n",
    "# triplets_data_train --> corresponding triplets for training (Y)\n",
    "# sents_data_validation --> texts for validation (X)\n",
    "# triplets_data_validation --> corresponding triplets for validation (Y)\n",
    "# sents_data_test --> texts for testing (X)\n",
    "# triplets_data_test --> corresponding triplets for testing (Y)\n",
    "# ======================================================================================================================\n",
    "\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import gensim.downloader\n",
    "\n",
    "# Load the datasets\n",
    "dataset = load_dataset(\"docred\")\n",
    "\n",
    "# Prepare the data\n",
    "sents_data = {}\n",
    "triplets_data = {}\n",
    "for key in dataset.keys():\n",
    "    sents_data[key] = []\n",
    "    triplets_data[key] = []\n",
    "    for row in dataset[key]:\n",
    "        triplets = []\n",
    "        for i in range(len(row['labels']['relation_text'])):\n",
    "            triplets.append([\"<triplet>\", row['vertexSet'][row['labels']['head'][i]][0]['name'], \"<subj>\", row['vertexSet'][row['labels']['tail'][i]][0]['name'], \"<obj>\", row['labels']['relation_text'][i]])\n",
    "        sents_data[key].append(row['sents'])\n",
    "        triplets_data[key].append(triplets)\n",
    "\n",
    "# Concatenate the annotated and distant data\n",
    "sents_data['train'] = sents_data['train_annotated'] + sents_data['train_distant']\n",
    "triplets_data['train'] = triplets_data['train_annotated'] + triplets_data['train_distant']\n",
    "\n",
    "# Convert the data to string format\n",
    "sents_data['train'] = [' '.join([' '.join(sent) for sent in sents]) for sents in sents_data['train']]\n",
    "sents_data['validation'] = [' '.join([' '.join(sent) for sent in sents]) for sents in sents_data['validation']]\n",
    "sents_data['test'] = [' '.join([' '.join(sent) for sent in sents]) for sents in sents_data['test']]\n",
    "triplets_data['train'] = [' '.join([' '.join(triplet) for triplet in triplets]) for triplets in triplets_data['train']]\n",
    "triplets_data['validation'] = [' '.join([' '.join(triplet) for triplet in triplets]) for triplets in triplets_data['validation']]\n",
    "triplets_data['test'] = [' '.join([' '.join(triplet) for triplet in triplets]) for triplets in triplets_data['test']]\n",
    "\n",
    "# Pickle the lists\n",
    "with open('data/triplets_data_train.pkl', 'wb') as f:\n",
    "    pickle.dump(triplets_data['train'], f)\n",
    "\n",
    "with open('data/triplets_data_validation.pkl', 'wb') as f:\n",
    "    pickle.dump(triplets_data['validation'], f)\n",
    "\n",
    "with open('data/triplets_data_test.pkl', 'wb') as f:\n",
    "    pickle.dump(triplets_data['test'], f)\n",
    "\n",
    "with open('data/sents_data_train.pkl', 'wb') as f:\n",
    "    pickle.dump(sents_data['train'], f)\n",
    "\n",
    "with open('data/sents_data_validation.pkl', 'wb') as f:\n",
    "    pickle.dump(sents_data['validation'], f)\n",
    "\n",
    "with open('data/sents_data_test.pkl', 'wb') as f:\n",
    "    pickle.dump(sents_data['test'], f)\n",
    "\n",
    "# Load pretrained \"word2vec\" embeddings\n",
    "w2v = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "# Pickle the w2v\n",
    "with open('data/w2v.pkl', 'wb') as f:\n",
    "    pickle.dump(w2v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('data/triplets_data_train.pkl', 'rb') as f:\n",
    "    triplets_data_train = pickle.load(f)\n",
    "\n",
    "with open('data/triplets_data_validation.pkl', 'rb') as f:\n",
    "    triplets_data_validation = pickle.load(f)\n",
    "\n",
    "with open('data/triplets_data_test.pkl', 'rb') as f:\n",
    "    triplets_data_test = pickle.load(f)\n",
    "\n",
    "with open('data/sents_data_train.pkl', 'rb') as f:\n",
    "    sents_data_train = pickle.load(f)\n",
    "\n",
    "with open('data/sents_data_validation.pkl', 'rb') as f:\n",
    "    sents_data_validation = pickle.load(f)\n",
    "\n",
    "with open('data/sents_data_test.pkl', 'rb') as f:\n",
    "    sents_data_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Zest Airways , Inc. operated as AirAsia Zest ( formerly Asian Spirit and Zest Air ) , was a low - cost airline based at the Ninoy Aquino International Airport in Pasay City , Metro Manila in the Philippines . It operated scheduled domestic and international tourist services , mainly feeder services linking Manila and Cebu with 24 domestic destinations in support of the trunk route operations of other airlines . In 2013 , the airline became an affiliate of Philippines AirAsia operating their brand separately . Its main base was Ninoy Aquino International Airport , Manila . The airline was founded as Asian Spirit , the first airline in the Philippines to be run as a cooperative . On August 16 , 2013 , the Civil Aviation Authority of the Philippines ( CAAP ) , the regulating body of the Government of the Republic of the Philippines for civil aviation , suspended Zest Air flights until further notice because of safety issues . Less than a year after AirAsia and Zest Air 's strategic alliance , the airline has been rebranded as AirAsia Zest . The airline was merged into AirAsia Philippines in January 2016 .\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# [word for word in sents_data_train[0].split() if word in w2v.vocab]\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sents_data_train[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m w2v\u001b[39m.\u001b[39mvocab]\n",
      "\u001b[1;32m/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# [word for word in sents_data_train[0].split() if word in w2v.vocab]\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sents_data_train[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m w2v\u001b[39m.\u001b[39mvocab]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/gensim/models/keyedvectors.py:734\u001b[0m, in \u001b[0;36mKeyedVectors.vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    733\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvocab\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 734\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    735\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe vocab attribute was removed from KeyedVector in Gensim 4.0.0.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse KeyedVector\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms .key_to_index dict, .index_to_key list, and methods \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    739\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "# [word for word in sents_data_train[0].split() if word in w2v.vocab]\n",
    "[word for word in sents_data_train[0].split() if word in w2v.index_to_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['triplet']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in w2v.index_to_key if 'triplet' == word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '<triplet>' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cansakiroglu/_dev/__DL_Project/ipynb.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m w2v[\u001b[39m'\u001b[39m\u001b[39m<triplet>\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key_or_keys)\n\u001b[1;32m    405\u001b[0m \u001b[39mreturn\u001b[39;00m vstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vector\u001b[39m(\u001b[39mself\u001b[39m, key, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(key)\n\u001b[1;32m    447\u001b[0m     \u001b[39mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key '<triplet>' not present\""
     ]
    }
   ],
   "source": [
    "w2v['<triplet>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.__setitem__(keys, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511\n",
      "510\n",
      "508\n",
      "1199\n",
      "551\n",
      "0\n",
      "So, we can set the max length of the input sequence to 600 and the max length of the output sequence to 1200.\n"
     ]
    }
   ],
   "source": [
    "print(max([len(x.split()) for x in sents_data_train]))\n",
    "print(max([len(x.split()) for x in sents_data_validation]))\n",
    "print(max([len(x.split()) for x in sents_data_test]))\n",
    "print(max([len(x.split()) for x in triplets_data_train]))\n",
    "print(max([len(x.split()) for x in triplets_data_validation]))\n",
    "print(max([len(x.split()) for x in triplets_data_test]))\n",
    "\n",
    "print('So, we can set the max length of the input sequence to 600 and the max length of the output sequence to 1200.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
